<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
    <title>Kaushik Subramanian</title>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-74348278-1', 'auto');
  ga('send', 'pageview');

</script>
  </head>
  <body>
    &nbsp;&nbsp;&nbsp; <br>
    <div align="center">&nbsp;&nbsp;<span style="color: black;
        -moz-text-blink: none; -moz-text-decoration-color:
        -moz-use-text-color; -moz-text-decoration-line: none;
        -moz-text-decoration-style: solid;">&nbsp; <a style="color:
          black; text-decoration: none;" href="index.html">Home</a>&nbsp;&nbsp;





        &nbsp;</span> &nbsp;<span style="color: black;">&nbsp;&nbsp;&nbsp;





        <a style="color: black; text-decoration: none;"
          href="experience.html">Experience</a>&nbsp;&nbsp; &nbsp;</span>&nbsp;&nbsp;<span
        style="color: black; background-color: rgb(204, 102, 0);">&nbsp;&nbsp;&nbsp;



        <a style="color: black; text-decoration: none;"
          href="research.html">Research</a>&nbsp;&nbsp; &nbsp;</span>&nbsp;<span
        style="color: black;"><span style="color: black;">&nbsp;&nbsp;
          &nbsp;</span> <span style="color: black;"><a style="color:
            black; text-decoration: none;"
            href="teaching/cs4641/index.html">Teaching</a></span>&nbsp;&nbsp;


        &nbsp; &nbsp; &nbsp;&nbsp; <a style="color: black;
          text-decoration: none;" href="publications.html">Publications</a></span><span
        style="color: black;"></span><br>
    </div>
    <hr size="1" width="700" noshade="noshade"><br>
    <div style="position: absolute; left: 50%; top: 70px; width: 700px;
      height: 2250px; margin-left: -350px;" align="justify"><br>
      <br>
      <font color="#cc6600"><big>Projects</big></font><br>
      <ul>
        <li><a name="Machine_Learning"></a><a href="#Machine_Learning">Machine


            Learning</a></li>
        <li><a href="research_Robo.html#Robotics">Robotics</a></li>
        <li><a href="research_CV.html#Computer_Vision">Computer Vision</a></li>
      </ul>
      <br>
      <big><font color="#cc6600">Machine Learning<br>
        </font></big>
      <hr size="1" width="700" noshade="noshade"><b><br>
        Sentiment Analysis (using Deep Learning) on Amazon Review Data</b><br>
      <div>
        <p style="float: left;">Advisor - Prof. Ling Liu,</p>
        <p style="float: right;">Jan 2015</p>
      </div>
      <br style="clear: both">
      Amazon product reviews from users are not always consistent with
      the numerical ratings provided. The ratings can be lower or higher
      than the overall sentiment the review text conveys. In this work,
      we use existing work in sentiment analysis using recurrent neural
      networks and apply it to text reviews from Amazon. Our goal is to
      provide the prospective buyer with an adjusted rating that agrees
      more with the sentiment of the text. We compare the adjusted
      ratings from our system and human predictions of the sentiment and
      find 70% agreement, encouraging further exploration.<br>
      <br>
      <big><font color="#cc6600"> </font></big>
      <hr size="1" width="700" noshade="noshade"><br>
      <b>Concept Learning using Interactive Clustering</b><br>
      <div>
        <p style="float: left;">Advisor - Prof. Charles Isbell &amp;
          Prof. Andrea Thomaz,</p>
        <p style="float: right;">May 2011 to July 2011</p>
      </div>
      <br style="clear: both">
      In this project we use concepts of Active Learning to interact and
      obtain labels from a human to learn simple concepts using
      clustering algorithms like kNN and GMMs. The concepts we dealt
      with included positional attributes of a table clean-up task (like
      sink, trash), attributes used to identify animals (like tail,
      fins, claws etc.) and attributes used to categorize people into
      social groups (friends, family, colleagues and so on).&nbsp; We
      would incrementally learn these concepts by asking the human
      specific questions and then expand our concept set by generating
      conjunctions and disjunctions to enable higher-level concepts.
      These combinations were randomly generated but constrained by the
      labels received as well as task-specific heuristics. This simple
      approach proved to work reasonably well to enable fast
      instantiation of complex concepts.<br>
      <br>
      <big><font color="#cc6600"> </font></big>
      <hr size="1" width="700" noshade="noshade"><br>
      <b>POMDP-based Planning for a Table-top Search and Find Task</b><br>
      <div>
        <p style="float: left;">Advisor - Prof. Mike Stilman (GAtech),</p>
        <p style="float: right;">August 2010 to December 2010</p>
      </div>
      <br style="clear: both">
      For robot mobile manipulators in human environments, an important
      task is object retrieval. We investigate the task of optimally
      locating and grasping a goal object in a cluttered environment. We
      model the world as a Partially Observable Markov Decision Process
      and use two forms of task representation. Our first design is a
      grid-based representation which takes different views of the scene
      and the uncertainty associated with robot vision. Our second
      approach attempts to exploit a more informed vision system. A tree
      of obstructing objects is gathered from the scene and planning is
      done over the possible tree configurations. We solve the POMDP
      using Point-based Value Iteration algorithms and evaluate the
      performance on few sample search scenarios.<br>
      <br>
      Download the report <a href="files/RIP_Final.pdf">here</a>.<br>
      <br>
      <hr size="1" width="700" noshade="noshade"><br>
      <b>Efficient Apprenticeship Learning with Smart Humans - AAAI
        Learning by Demonstration Challenge</b><br>
      <div>
        <p style="float: left;">Advisor - Prof. Michael Littman
          (Rutgers),</p>
        <p style="float: right;">April 2010 to July 2010</p>
      </div>
      <br style="clear: both">
      We develop a generalized apprenticeship learning protocol for
      reinforcement-learning agents with access to a teacher. The
      teacher interacts with the agent by providing policy traces
      (transition and reward observations). We characterize sufficient
      conditions of the underlying models for efficient apprenticeship
      learning and link this criteria to two established learnability
      classes (KWIK and Mistake Bound). <br>
      <br>
      We demonstrate our approach in a conjunctive learning task that
      would be too slow to learn in the autonomous setting. We show that
      the agent can guarantee near-optimal performance with only a
      polynomial number of examples from a human teacher and can
      efficiently learn in real world environments with sensor
      imprecision and stochasticity.<br>
      <br>
      Compiled video <a
        href="http://www.youtube.com/watch?v=nt4RRd5A40E">here</a>. <br>
      It describes, 1. Autonomous Model Learning, 2. Human Interaction
      and 3. Execution of the learned policy.<br>
      <br>
      Download the report <a href="files/aaai_lbd_tech_report.pdf">here</a>.<br>
      <br>
      <hr size="1" width="700" noshade="noshade"><br>
      <b>State Space Abstraction with the Highway Car Domain</b><br>
      <div>
        <p style="float: left;">Advisor - Prof. Michael Littman
          (Rutgers),</p>
        <p style="float: right;">August 2009 to December 2009</p>
      </div>
      <br style="clear: both">
      In this project, we use the help of an expert human to learn the
      task of navigating on a simulated highway. We take advantage of
      the different forms of input that can be given by the human and
      map them to the agent's world. The human interaction could be in
      one of two ways - by providing rewards or by providing a policy.
      We introduce a novel approach where the humans provides high level
      state abstractions. The criteria used by the human was - "states
      are similar if the same optimal action is to performed in both the
      states". This interactive abstraction significantly sped-up the
      performance of the agent. <br>
      <br>
      Simulated video <a
        href="http://www.youtube.com/watch?v=FnPgz-Fefxo">here</a>.<br>
      Details can be found in Chapter 4 of my Thesis. <br>
      <br>
      <hr size="1" width="700" noshade="noshade"><br>
      <b>Introduction to Model-based Reinforcement Learning - IJCAI
        2009, Best Narration Award</b><br>
      <div>
        <p style="float: left;">Advisor - Prof. Michael Littman
          (Rutgers),</p>
        <p style="float: right;">Feb 2009 to April 2009</p>
      </div>
      <br style="clear: both">
      We created a reinforcement-learning demo -- a simple robot
      navigation task -- and took it to the public to teach them about
      AI and robotics. The video shows the system adapting in real time
      to various modifications to the robot's design and provides a very
      gentle introduction to the idea of model-based reinforcement
      learning.<br>
      <br style="color: rgb(204, 204, 204);">
      You can find the video <a
        href="http://videolectures.net/ijcai09_littman_rlrl/">here</a>.<br>
      <br>
      <hr size="1" width="700" noshade="noshade"><br>
      <b>Multi-Dimensional Particle Swarm Optimization - A Parallel
        Approach</b><br>
      <div>
        <p style="float: left;">Advisor - Prof. Manish Parashar
          (Rutgers),</p>
        <p style="float: right;">Oct 2008 to Dec 2008</p>
      </div>
      <br style="clear: both">
      Particle Swarm Optimization (PSO) is a population-based stochastic
      algorithm that deals with real world optimization problems. When
      using this technique to optimize multi-dimensional functions, a
      large amount of computational power is required to achieve
      efficient results. To alleviate this issue a parallel algorithm
      was implemented in an MPI cluster. The advantage of this was it
      produced the same result as that obtained from the serial
      algorithm with greatly reduced computation time and increased
      scalability. Large swarm population sizes can be easily managed
      when using multiple processors.<br>
      <br>
      <br>
      <br>
    </div>
  </body>
</html>
