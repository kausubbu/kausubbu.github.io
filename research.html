<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <meta content="text/html; charset=windows-1252"
      http-equiv="Content-Type">
    <title>Kaushik Subramanian</title>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-74348278-1', 'auto');
  ga('send', 'pageview');

</script>
  </head>
  <body>
    &nbsp; <br>
    <div align="center">&nbsp;&nbsp;<span style="color: black;
        -moz-text-blink: none; -moz-text-decoration-color:
        -moz-use-text-color; -moz-text-decoration-line: none;
        -moz-text-decoration-style: solid;">&nbsp; <a style="color:
          black; text-decoration: none;" href="index.html">Home</a>&nbsp;&nbsp;



        &nbsp;</span> &nbsp;<span style="color: black;">&nbsp;&nbsp;&nbsp;



        <a style="color: black; text-decoration: none;"
          href="experience.html">Experience</a>&nbsp;&nbsp; &nbsp;</span>&nbsp;&nbsp;<span
        style="color: black; background-color: rgb(204, 102, 0);">&nbsp;&nbsp;&nbsp;


        <a style="color: black; text-decoration: none;"
          href="research.html">Research</a>&nbsp;&nbsp; &nbsp;</span>&nbsp;


      <span style="color: black;">&nbsp;</span><span style="color:
        black;"><span style="color: black;">&nbsp;&nbsp; </span><span
          style="color: black;"><a style="color: black; text-decoration:
            none;" href="teaching/cs4641/index.html">Teaching</a></span>
        &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; <a style="color: black;
          text-decoration: none;" href="publications.html">Publications</a></span><span
        style="color: black;"></span><br>
    </div>
    <hr size="1" width="700" noshade="noshade"><br>
    <div style="position: absolute; left: 50%; top: 70px; width: 700px;
      height: 750; margin-left: -350px;" align="justify"><br>
      <br>
      <font color="#cc6600"><big>Projects</big></font><br>
      <ul>
        <li><a href="research_ML.html#Machine_Learning">Machine Learning</a></li>
        <li><a href="research_Robo.html#Robotics">Robotics</a></li>
        <li><a href="research_CV.html#Computer_Vision">Computer Vision</a></li>
      </ul>
      <big><font color="#cc6600"><br>
          Doctoral Dissertation<br>
          <br>
        </font></big><font color="#cc6600"><font color="#000000"><font
            color="#cc6600"><font color="#000000">My dissertation is
              titled "<b>Policy-based Exploration for Efficient
                Reinforcement Learning</b>". My advisors are Prof.
              Charles Isbell and Prof. Andrea Thomaz (Interactive
              Computing, Georgia Tech)<br>
              <br>
              Abstract - <br>
              <br>
              Reinforcement Learning (RL) is the field of research
              focused on solving sequential decision- making tasks
              modeled as Markov Decision Processes. Researchers have
              shown RL to be successful at solving a variety of problems
              like system operations (logistics), robot tasks (soccer,
              helicopter control) and computer games (Go, backgammon);
              however, in general, standard RL approaches do not scale
              well with the size of the problem. The reason this problem
              arises is that RL approaches rely on obtaining samples
              useful for learning the underlying structure. In this work
              we tackle the problem of smart exploration in RL,
              autonomously and using human interaction. We propose
              policy-based methods that serve to effectively bias
              exploration towards important aspects of the domain.<br>
              <br>
              Reinforcement Learning agents use function approximation
              methods to generalize over large and complex domains. One
              of the most well-studied approaches is using linear
              regression algorithms to model the value function of the
              decision-making problem. We introduce a policy-based
              method that uses statistical criteria derived from linear
              regression analysis to bias the agent to explore samples
              useful for learning. We show how we can learn exploration
              policies autonomously and from human demonstrations (using
              concepts of active learning) to facilitate fast
              convergence to the optimal policy. We then tackle the
              problem of human-guided exploration in RL. We present a
              probabilistic method which combines human evaluations,
              instantiated as policy signals, with Bayesian RL. We show
              how this approach provides performance speedups while
              being robust to noisy, suboptimal human signals. We also
              present an approach that makes use of some of the inherent
              structure in the exploratory human demonstrations to
              assist Monte Carlo RL to overcome its limitations and
              efficiently solve large-scale problems. We implement our
              methods on popular arcade games and highlight the
              improvements achieved using our approach. We show how the
              work on using humans to help agents efficiently explore
              sequential decision-making tasks is an important and
              necessary step in applying Reinforcement Learning to
              complex problems.<br>
              <br>
              You can find a copy of the dissertation <a
                moz-do-not-send="true"
                href="Kaushik_Subramanian_Doctoral_Dissertation.pdf">here</a>.<br>
              <br>
            </font></font></font></font><big><font color="#cc6600"><br>
        </font></big> <big><font color="#cc6600">Master's Dissertation<br>
        </font></big><br>
      My thesis is titled "<b>HELP - Human assisted Efficient Learning
        Protocols</b>". My advisor is Prof. Michael Littman (CS
      department, Rutgers University) and Prof. Zoran Gajic (ECE
      department, Rutgers University).<br>
      <br>
      Abstract -<br>
      <br>
      In recent years, there has been a growing attention towards the
      development of artificial agents that can naturally communicate
      and interact with humans. The focus has primarily been on creating
      systems that have the ability to unify advanced learning
      algorithms along with various natural forms of human interaction
      (like providing advice, guidance, motivation, punishment, etc).
      However, despite the progress made, interactive systems are still
      directed towards researchers and scientists and consequently the
      everyday human is unable to exploit the potential of these
      systems. Another undesirable component is that in most cases, the
      interacting human is required to communicate with the artificial
      agent a large number of times, making the human often fatigued. In
      order to improve these systems, this thesis extends prior work and
      introduces novel approaches via Human-assisted Efficient Learning
      Protocols (HELP). Three case studies are presented that detail
      distinct aspects of HELP - a) representation of the task to be
      learned and its associated constraints, b) the efficiency of the
      learning algorithm used by the artificial agent and c) the
      unexplored "natural" modes of human interaction. The case studies
      will show how an artificial agent is able to efficiently learn
      and perform complex tasks using only a limited number of
      interactions with a human. Each of these studies involves humans
      subjects interacting with a real robot and/or simulated agent to
      learn a particular task. The focus of HELP is to show that a
      machine can learn better from humans if it is given the ability to
      take advantage of the knowledge provided by interacting with a
      human partner or teacher.<br>
      <br>
      You can find a copy of the dissertation <a
        href="files/Thesis_Kaushik.pdf">here</a>.<br>
      <br>
      <br>
    </div>
  </body>
</html>
