<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <meta content="text/html; charset=ISO-8859-1"
      http-equiv="Content-Type">
    <title>Kaushik Subramanian</title>
  </head>
  <body alink="#EE0000" bgcolor="#000000" link="#3366ff" text="#ffffff"
    vlink="#551A8B">
    &nbsp;&nbsp;&nbsp; <br>
    <div align="center"> &nbsp;&nbsp;&nbsp; <a style="color: white;"
        href="index.html">Home</a>&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;
      <a style="color: white;" href="experience.html">Experience</a>&nbsp;&nbsp;

      &nbsp;&nbsp;&nbsp;&nbsp; <a style="color: white;"
        href="research.html"> Research</a>&nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; <a style="color: white;"
        href="publication.html">Publications</a>&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp;&nbsp; <a style="color: white;"
        href="files/Resume_KaushikSubramanian.pdf">Resume</a>&nbsp;&nbsp;&nbsp;




      <br>
    </div>
    <hr noshade="noshade" size="1" width="700"><br>
    <div style="position: absolute; left: 50%; top: 70px; width: 700px;
      height: 2952px; margin-left: -350px;" align="justify"><br>
      <br>
      <big><font color="#cc6600">Master's Dissertation<br>
        </font></big><br>
      My thesis is titled "<font color="#990000">HELP - Human assisted
        Efficient Learning Protocols</font>". My advisor is Prof.
      Michael Littman (Rutgers CS department) and Prof. Zoran Gajic
      (Rutgers ECE Department).<br>
      <br>
      Abstract -<br>
      <br>
      In recent years, there has been a growing attention towards the
      development of artificial agents that can naturally communicate
      and interact with humans. The focus has primarily been on creating
      systems that have the ability to unify advanced learning
      algorithms along with various natural forms of human interaction
      (like providing advice, guidance, motivation, punishment, etc).
      However, despite the progress made, interactive systems are still
      directed towards researchers and scientists and consequently the
      everyday human is unable to exploit the potential of these
      systems. Another undesirable component is that in most cases, the
      interacting human is required to communicate with the artificial
      agent a large number of times, making the human often fatigued. In
      order to improve these systems, this thesis extends prior work and
      introduces novel approaches via Human-assisted Efficient Learning
      Protocols (HELP). Three case studies are presented that detail
      distinct aspects of HELP - a) representation of the task to be
      learned and its associated constraints, b) the efficiency of the
      learning algorithm used by the artificial agent and c) the
      unexplored "natural" modes of human interaction. The case studies
      will show how an artificial agent is able to efficiently learn
      and perform complex tasks using only a limited number of
      interactions with a human. Each of these studies involves humans
      subjects interacting with a real robot and/or simulated agent to
      learn a particular task. The focus of HELP is to show that a
      machine can learn better from humans if it is given the ability to
      take advantage of the knowledge provided by interacting with a
      human partner or teacher.<br>
      <br>
      You can find a copy of the dissertation <a
href="file:///C:/Users/kausubbu/Dropbox/Misc/Website/files/Thesis_Kaushik.pdf">here</a>.<br>
      <br>
      <br>
      <big><font color="#cc6600">Selected Projects</font></big><br>
      <hr noshade="noshade" size="1" width="700"> <b><br>
      </b><font color="#990000"> Robot Learning from Demonstration:
        Kinesthetic Teaching vs. Teleoperation</font><br>
      <br>
      Advisor - Prof. Andrea Thomaz (GAtech),&nbsp; &nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;
      March 2011 to May 2011<br>
      <br>
      Abstract - <br>
      <br>
      We are interested in developing learning from demonstration
      systems that are suitable to be used by everyday people. We
      compare two interaction methods, kinesthetic teaching and
      teleoperation, for the users to show successful demonstrations of
      a skill. In the former, the user physically guides the robot and
      in the latter the user controls the robot with a haptic device. We
      evaluate our results using skill dependent quantitative measures,
      timing information and survey questions. We find that kinesthetic
      teaching is faster in terms of giving a single demonstration and
      the demonstrations are more successful. However, the learned skill
      does not perform better as expected. The survey results show that
      users think kinesthetic teaching is easier and more accurate and
      an open-ended question suggests that people would prefer
      kinesthetic teaching over teleoperation for everyday skills.<br>
      <br>
      Download the report <a
        href="file:///C:/Users/kausubbu/Dropbox/Misc/Website/files/HRIFinalBK">here</a>.<br>
      <br>
      <hr noshade="noshade" size="1" width="700"><br>
      <font color="#990000">MDP-based Planning for a Table-top Search
        and Find Task<br>
      </font> <br>
      Advisor - Prof. Mike Stilman (GAtech),&nbsp; &nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; August 2010 to December 2010<br>
      <br>
      Abstract - <br>
      <br>
      For robot mobile manipulators in human environments, an important
      task is object retrieval. We investigate the task of optimally
      locating and grasping a goal object in a cluttered environment. We
      model the world as a Partially Observable Markov Decision Process
      and use two forms of task representation. Our first design is a
      grid-based representation which takes different views of the scene
      and the uncertainty associated with robot vision. Our second
      approach attempts to exploit a more informed vision system. A tree
      of obstructing objects is gathered from the scene and planning is
      done over the possible tree configurations. We solve the POMDP
      using Point-based Value Iteration algorithms and evaluate the
      performance on few sample search scenarios.<br>
      <br>
      Download the report <a
href="file:///C:/Users/kausubbu/Dropbox/Misc/Website/files/RIP_Final.pdf">here</a>.<br>
      <br>
      <hr noshade="noshade" size="1" width="700"><br>
      <font color="#990000">AAAI Learning by Demonstration Challenge -
        Efficient Apprenticeship Learning with Smart Humans<br>
      </font> <br>
      Advisor - Prof. Michael Littman (Rutgers),&nbsp; &nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;







      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; April 2010 to July
      2010<br>
      <br>
      Abstract - <br>
      <br>
      We develop a generalized apprenticeship learning protocol for
      reinforcement-learning agents with access to a teacher. The
      teacher interacts with the agent by providing policy traces
      (transition and reward observations). We characterize sufficient
      conditions of the underlying models for efficient apprenticeship
      learning and link this criteria to two established learnability
      classes (KWIK and Mistake Bound). <br>
      <br>
      We demonstrate our approach in a conjunctive learning task that
      would be too slow to learn in the autonomous setting. We show that
      the agent can guarantee near-optimal performance with only a
      polynomial number of examples from a human teacher and can
      efficiently learn in real world environments with sensor
      imprecision and stochasticity.<br>
      <br>
      Experimental video <a
        href="http://www.youtube.com/watch?v=nt4RRd5A40E">here</a>. The
      video describes, 1. Autonomous Model Learning, 2. Human
      Interaction and 3. Execution of the learned policy.<br>
      <br>
      Download the report <a
href="file:///C:/Users/kausubbu/Dropbox/Misc/Website/files/aaai_lbd_tech_report.pdf">here</a>.<br>
      <br>
      <hr noshade="noshade" size="1" width="700"><br>
      <font color="#990000">Novel Interactive Learning with the Highway
        Car Domain</font><br>
      <br>
      Advisor - Prof. Michael Littman (Rutgers),&nbsp; &nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; August
      2009 to December 2009<br>
      <br>
      Abstract -<br>
      <br>
      In this project, we use the help of an expert human to learn the
      task of navigating on a simulated highway. We take advantage of
      the different forms of input that can be given by the human and
      map them to the agent's world. The human interaction could be in
      one of two ways - by providing rewards or by providing a policy.
      We introduce a novel approach where the humans provides high level
      state abstractions. The criteria used by the human was - "states
      are similar if the same optimal action is to performed in both the
      states". This interactive abstraction significantly sped-up the
      performance of the agent.<br>
      <br>
      Details can be found in Chapter 4 of my Thesis. Simulated video <a
        href="http://www.youtube.com/watch?v=FnPgz-Fefxo">here</a>.<br>
      <br>
      <hr noshade="noshade" size="1" width="700"><br>
      <font color="#990000"> Humanoid Robot Learning using Gaussian
        Mixture Models<br>
      </font> <br>
      Advisor - Prof. Gerhard Lakemeyer (RWTH),&nbsp; &nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp; May 2009 to August 2009<br>
      <br>
      Internship at the <a
        href="http://www-kbsg.informatik.rwth-aachen.de/">Knowledge
        Based Systems Group</a> at RWTH University in Germany.<br>
      <br>
      Abstract -<br>
      <br>
      A system was developed for robot behavior acquisition using
      kinesthetic demonstrations. It enables a humanoid robot to imitate
      constrained reaching gestures directed towards a target using a
      learning algorithm based on Gaussian Mixture Regression. The
      imitation trajectory can be reshaped in order to satisfy the
      constraints of the task and it can adapt to changes in the initial
      conditions and to target displacements occurring during the
      movement execution. The potential of this method was evaluated
      using experiments involving Aldebaran's Nao humanoid robot and
      Fawkes, an open source robot software by the KBSG at RWTH
      University.<br>
      <br>
      You can find a video of the Demonstrations <a
        href="http://www.youtube.com/watch?v=18LFdPGhOoI">here.</a>
      Download the report <a
href="file:///C:/Users/kausubbu/Dropbox/Misc/Website/files/robot_learning_kaushik.pdf">here</a>.<br>
      <br>
      Update - <br>
      <br>
      When teaching the Nao complex behaviors that involved using many
      actuators, the Gaussian Mixture Regression (GMR) model was found
      to be very slow and not suitable for online applications. In order
      to overcome this, the Approximate Nearest Neighbour Search (ANNS)
      algorithm was tested as a regression model in the Nao Simulator
      (Webots). The time complexity and the prediction errors were
      analyzed and a graph of their performance against the GMR can be
      found <a
href="file:///C:/Users/kausubbu/Dropbox/Misc/Website/image/anns_gmr_analysis.jpg">here</a>.<br>
      <br>
      <hr noshade="noshade" size="1" width="700"><br>
      <font color="#990000"> IJCAI 2009, Best Narration Award -
        Introduction to Model-based Reinforcement Learning</font><br>
      <br>
      Advisor - Prof. Michael Littman (Rutgers),&nbsp; &nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;
      &nbsp;&nbsp;&nbsp; Feb 2009 to April 2009<br>
      <br style="color: rgb(204, 204, 204);">
      We created a reinforcement-learning demo -- a simple robot
      navigation task -- and took it to the public to teach them about
      AI and robotics. The video shows the system adapting in real time
      to various modifications to the robot's design and provides a very
      gentle introduction to the idea of model-based reinforcement
      learning.<br>
      <br style="color: rgb(204, 204, 204);">
      You can find the video <a
        href="http://videolectures.net/ijcai09_littman_rlrl/">here</a>.<br>
    </div>
  </body>
</html>
